import pdb
from transformers import AutoTokenizer
from datasets import load_dataset
import numpy as np
import torch
import random

def set_seed(seed):
    random.seed(seed)
    np.random.seed(seed)
    torch.random.manual_seed(seed)

def format_piqa(ex):
    goal = ex["goal"].strip()
    sol1 = ex["sol1"].strip()
    sol2 = ex["sol2"].strip()
    # PIQAëŠ” 2-choiceë¼ A/Bë¡œ ë‘ëŠ” ê²Œ ê¹”ë”
    return (
        f"Goal: {goal}\n"
        f"Choices:\n"
        f"A. {sol1}\n"
        f"B. {sol2}\n"
        f"Answer:"
    )

def format_winograde(ex):
    # winogrande: sentenceì— '_' ë¹ˆì¹¸ì´ ìžˆê³  option1/2 ì¤‘ í•˜ë‚˜ë¡œ ì±„ì›€
    sent = ex["sentence"].strip()
    opt1 = ex["option1"].strip()
    opt2 = ex["option2"].strip()
    return (
        f"Sentence: {sent}\n"
        f"Choices:\n"
        f"A. {opt1}\n"
        f"B. {opt2}\n"
        f"Answer:"
    )

def format_boolq(ex):
    passage = ex["passage"].strip()
    question = ex["question"].strip()

    return (
        f"Passage: {passage}\n"
        f"Question: {question}\n"
        f"Answer:"
    )

def format_arc(ex):
    q = ex["question"].strip()
    texts = ex["choices"]["text"]
    labels = ex["choices"]["label"]

    opts = "\n".join([f"{l}. {t}" for l, t in zip(labels, texts)])
    return f"Question: {q}\nChoices:\n{opts}\nAnswer:"

def format_hellaswag(ex):
    ctx = (ex["ctx_a"] + " " + ex["ctx_b"]).strip()
    endings = ex["endings"]
    labels = ["A", "B", "C", "D"]

    opts = "\n".join([f"{labels[i]}. {endings[i]}" for i in range(4)])
    return f"Context: {ctx}\nEndings:\n{opts}\nAnswer:"

def get_pile(nsamples, seed, seqlen, model, tokenizer=None):
    print("get_pile")
    traindata = load_dataset("json", data_files='/path/to/val.jsonl.zst', split="train")
    if tokenizer is None:
        tokenizer = AutoTokenizer.from_pretrained(model, use_fast=False)
    trainenc = tokenizer("\n\n".join(traindata['text'][:1000]), return_tensors='pt')

    random.seed(seed)
    trainloader = []
    for _ in range(nsamples):
        i = random.randint(0, trainenc.input_ids.shape[1] - seqlen - 1)
        j = i + seqlen
        inp = trainenc.input_ids[:, i:j]
        tar = inp.clone()
        tar[:, :-1] = -100
        trainloader.append((inp, tar))
    return trainloader, None


def get_wikitext2(nsamples, seed, seqlen, model, tokenizer=None):
    print("get_wikitext2")
    traindata = load_dataset('Salesforce/wikitext', 'wikitext-2-raw-v1', split='train')
    testdata = load_dataset('Salesforce/wikitext', 'wikitext-2-raw-v1', split='test')
    if tokenizer is None:
        print(model)
        tokenizer = AutoTokenizer.from_pretrained(model, use_fast=False)
    trainenc = tokenizer("\n\n".join(traindata['text']), return_tensors='pt')
    testenc = tokenizer("\n\n".join(testdata['text']), return_tensors='pt')

    
    random.seed(seed)
    trainloader = []
    for _ in range(nsamples):
        i = random.randint(0, trainenc.input_ids.shape[1] - seqlen - 1)
        j = i + seqlen
        inp = trainenc.input_ids[:, i:j]
        tar = inp.clone()
        tar[:, :-1] = -100
        trainloader.append((inp, tar))
    return trainloader, testenc

def get_ptb(nsamples, seed, seqlen, model, tokenizer=None):
    print("get_ptb")
    traindata = load_dataset('ptb_text_only', 'penn_treebank', split='train')
    valdata = load_dataset('ptb_text_only', 'penn_treebank', split='validation')

    if tokenizer is None:
        tokenizer = AutoTokenizer.from_pretrained(model, use_fast=False)

    trainenc = tokenizer("\n\n".join(traindata['sentence']), return_tensors='pt')
    testenc = tokenizer("\n\n".join(valdata['sentence']), return_tensors='pt')

    random.seed(seed)
    trainloader = []
    for _ in range(nsamples):
        i = random.randint(0, trainenc.input_ids.shape[1] - seqlen - 1)
        j = i + seqlen
        inp = trainenc.input_ids[:, i:j]
        tar = inp.clone()
        tar[:, :-1] = -100
        trainloader.append((inp, tar))
    return trainloader, testenc

def get_c4(nsamples, seed, seqlen, model, tokenizer=None):
    print("get_c4")
    traindata = load_dataset(
        'json', data_files={'train': '/path/to/c4/en/c4-train.00000-of-01024.json.gz'}, split='train'
    )
    valdata = load_dataset(
        'json', data_files={'validation': '/path/to/c4/en/c4-validation.00000-of-00008.json.gz'}, split='validation'
    )

    if tokenizer is None:
        tokenizer = AutoTokenizer.from_pretrained(model, use_fast=False)

    random.seed(seed)
    trainloader = []
    for _ in range(nsamples):
        while True:
            i = random.randint(0, len(traindata) - 1)
            trainenc = tokenizer(traindata[i]['text'], return_tensors='pt')
            if trainenc.input_ids.shape[1] > seqlen:
                break
        i = random.randint(0, trainenc.input_ids.shape[1] - seqlen - 1)
        j = i + seqlen
        inp = trainenc.input_ids[:, i:j]
        tar = inp.clone()
        tar[:, :-1] = -100
        trainloader.append((inp, tar))

    random.seed(0)
    valenc = []
    for _ in range(256):
        while True:
            i = random.randint(0, len(valdata) - 1)
            tmp = tokenizer(valdata[i]['text'], return_tensors='pt')
            if tmp.input_ids.shape[1] >= seqlen:
                break
        i = random.randint(0, tmp.input_ids.shape[1] - seqlen - 1)
        j = i + seqlen
        valenc.append(tmp.input_ids[:, i:j])
    valenc = torch.hstack(valenc)

    return trainloader, valenc 

def get_ptb_new(nsamples, seed, seqlen, model, tokenizer=None):
    print("get_ptb_new")
    traindata = load_dataset('ptb_text_only', 'penn_treebank', split='train')
    testdata  = load_dataset('ptb_text_only', 'penn_treebank', split='test')

    if tokenizer is None:
        tokenizer = AutoTokenizer.from_pretrained(model, use_fast=False)

    trainenc = tokenizer(" ".join(traindata["sentence"]), return_tensors="pt")
    testenc = tokenizer(" ".join(testdata ["sentence"]), return_tensors="pt")

    random.seed(seed)
    trainloader = []
    for _ in range(nsamples):
        i = random.randint(0, trainenc.input_ids.shape[1] - seqlen - 1)
        j = i + seqlen
        inp = trainenc.input_ids[:, i:j]
        tar = inp.clone()
        tar[:, :-1] = -100
        trainloader.append((inp, tar))
    return trainloader, testenc


def get_c4_new(nsamples, seed, seqlen, model, tokenizer=None):
    print("get_c4_new")
    traindata = load_dataset(
        'json', data_files={'train': '/path/to/c4/en/c4-train.00000-of-01024.json.gz'}, split='train'
    )
    valdata = load_dataset(
        'json', data_files={'validation': '/path/to/c4/en/c4-validation.00000-of-00008.json.gz'}, split='validation'
    )
    if tokenizer is None:
        tokenizer = AutoTokenizer.from_pretrained(model, use_fast=False)
    
    random.seed(seed)
    trainloader = []
    for _ in range(nsamples):
        while True:
            i = random.randint(0, len(traindata) - 1)
            trainenc = tokenizer(traindata[i]["text"], return_tensors="pt")
            if trainenc.input_ids.shape[1] >= seqlen:
                break
        i = random.randint(0, trainenc.input_ids.shape[1] - seqlen - 1)
        j = i + seqlen
        inp = trainenc.input_ids[:, i:j]
        tar = inp.clone()
        tar[:, :-1] = -100
        trainloader.append((inp, tar))

    valenc = tokenizer(" ".join(valdata[:1100]["text"]), return_tensors="pt")
    valenc = valenc.input_ids[:, : (256 * seqlen)]
    return trainloader, valenc

def get_arc(nsamples, seqlen, tokenizer, seed=0, eval_mode=False, subset="challenge"):
    split = "train" if not eval_mode else "validation"
    random.seed(seed)

    subset = subset.lower()
    config = "ARC-Challenge" if subset in ["challenge", "arc-challenge"] else "ARC-Easy"

    ds = load_dataset("ai2_arc", config, split=split)

    trainloader = []
    for _ in range(nsamples):
        idx = random.randint(0, len(ds) - 1)
        enc = tokenizer(format_arc(ds[idx]), return_tensors="pt")

        L = enc.input_ids.shape[1]
        if L >= seqlen:
            i = random.randint(0, L - seqlen)
            inp = enc.input_ids[:, i:i + seqlen]
        else:
            inp = torch.nn.functional.pad(
                enc.input_ids,
                (0, seqlen - L),
                value=tokenizer.pad_token_id or 0,
            )

        tar = inp.clone()
        tar[:, :-1] = -100
        trainloader.append((inp, tar))

    return trainloader

def get_hellaswag(nsamples, seqlen, tokenizer, seed=0, eval_mode=False):
    split = "train" if not eval_mode else "validation"
    ds = load_dataset("hellaswag", split=split)

    random.seed(seed)

    trainloader = []
    for _ in range(nsamples):
        idx = random.randint(0, len(ds) - 1)
        enc = tokenizer(format_hellaswag(ds[idx]), return_tensors="pt")

        L = enc.input_ids.shape[1]
        if L >= seqlen:
            i = random.randint(0, L - seqlen)
            inp = enc.input_ids[:, i:i + seqlen]
        else:
            inp = torch.nn.functional.pad(
                enc.input_ids,
                (0, seqlen - L),
                value=tokenizer.pad_token_id or 0,
            )

        tar = inp.clone()
        tar[:, :-1] = -100
        trainloader.append((inp, tar))

    return trainloader

def get_boolq(nsamples, seqlen, tokenizer, seed=0, eval_mode=False):
    split = "train" if not eval_mode else "validation"
    ds = load_dataset("boolq", split=split)

    if eval_mode:
        return ds  # ðŸ”¥ í…ì„œ ë§ê³  raw dataset ë°˜í™˜

    random.seed(seed)
    # (adapt/trainloaderëŠ” ê¸°ì¡´ì²˜ëŸ¼ ìœ ì§€)
    trainloader = []
    for _ in range(nsamples):
        idx = random.randint(0, len(ds) - 1)
        enc = tokenizer(format_boolq(ds[idx]), return_tensors="pt")
        L = enc.input_ids.shape[1]
        if L >= seqlen:
            i = random.randint(0, L - seqlen)
            inp = enc.input_ids[:, i:i + seqlen]
        else:
            inp = torch.nn.functional.pad(
                enc.input_ids,
                (0, seqlen - L),
                value=tokenizer.pad_token_id or 0,
            )
        tar = inp.clone()
        tar[:, :-1] = -100
        trainloader.append((inp, tar))
    return trainloader

def get_winogrande(nsamples, seqlen, tokenizer, seed=0, eval_mode=False, config="winogrande_xl"):
    # HF: dataset nameì€ "winogrande", configëŠ” ë³´í†µ "winogrande_xl"ì„ ë§Žì´ ì”€
    split = "train" if not eval_mode else "validation"
    ds = load_dataset("winogrande", config, split=split)

    if eval_mode:
        # accuracy_evalì—ì„œ sentence/option1/option2/answerë¥¼ ì“°ê¸° ì¢‹ê²Œ raw ê·¸ëŒ€ë¡œ ë°˜í™˜
        return ds

    random.seed(seed)
    trainloader = []
    for _ in range(nsamples):
        idx = random.randint(0, len(ds) - 1)
        enc = tokenizer(format_winograde(ds[idx]), return_tensors="pt")

        L = enc.input_ids.shape[1]
        if L >= seqlen:
            i = random.randint(0, L - seqlen)
            inp = enc.input_ids[:, i:i + seqlen]
        else:
            inp = torch.nn.functional.pad(
                enc.input_ids,
                (0, seqlen - L),
                value=tokenizer.pad_token_id or 0,
            )

        tar = inp.clone()
        tar[:, :-1] = -100
        trainloader.append((inp, tar))

    return trainloader

def get_piqa(nsamples, seqlen, tokenizer, seed=0, eval_mode=False):
    split = "train" if not eval_mode else "validation"
    ds = load_dataset("baber/piqa", split="train", trust_remote_code=True)

    if eval_mode:
        # accuracy_evalì—ì„œ goal/sol1/sol2/label ê·¸ëŒ€ë¡œ ì“°ëŠ” ê²Œ ì œì¼ íŽ¸í•¨
        return ds

    random.seed(seed)
    trainloader = []
    for _ in range(nsamples):
        idx = random.randint(0, len(ds) - 1)
        enc = tokenizer(format_piqa(ds[idx]), return_tensors="pt")

        L = enc.input_ids.shape[1]
        if L >= seqlen:
            i = random.randint(0, L - seqlen)
            inp = enc.input_ids[:, i:i + seqlen]
        else:
            inp = torch.nn.functional.pad(
                enc.input_ids,
                (0, seqlen - L),
                value=tokenizer.pad_token_id or 0,
            )

        tar = inp.clone()
        tar[:, :-1] = -100
        trainloader.append((inp, tar))

    return trainloader

def get_loaders(
    name, nsamples=128, seed=0, seqlen=2048, model='', tokenizer=None
):
    if tokenizer is None:
        tokenizer = AutoTokenizer.from_pretrained(model, use_fast=False)
        
    if 'wikitext2' in name:
        return get_wikitext2(nsamples, seed, seqlen, model, tokenizer)
    if 'pile' in name:
        return get_pile(nsamples, seed, seqlen, model, tokenizer)
    if 'ptb' in name:
        if 'new' in name:
            return get_ptb_new(nsamples, seed, seqlen, model, tokenizer)  
        return get_ptb(nsamples, seed, seqlen, model, tokenizer)
    if 'c4' in name:
        if 'new' in name:
            return get_c4_new(nsamples, seed, seqlen, model, tokenizer)  
        return get_c4(nsamples, seed, seqlen, model, tokenizer)
    if name.lower() == "arc_c":
        data = get_arc(nsamples, seqlen, tokenizer, seed=seed, subset="challenge")
    if name.lower() == "arc_e":
        data = get_arc(nsamples, seqlen, tokenizer, seed=seed, subset="easy")
    if name.lower() == "hellaswag":
        data = get_hellaswag(nsamples, seqlen, tokenizer, seed=seed)
    if name.lower() == "boolq":
        data = get_boolq(nsamples, seqlen, tokenizer, seed=seed)
    if name.lower() == "piqa":
        data = get_piqa(nsamples, seqlen, tokenizer, seed=seed)      
    if name.lower() == "winogrande":
        data = get_winogrande(nsamples, seqlen, tokenizer, seed=seed)
    
    if name.lower() in ["arc_c", "arc_e", "hellaswag", "boolq", "piqa", "winogrande"]:
        return data, None
    if name.lower() == "mix":
        wiki_train,wiki_val=get_wikitext2(nsamples//4, seed, seqlen, model, tokenizer)
        boolq_train=get_boolq(nsamples//4, seqlen, tokenizer, seed=seed)
        piqa_train=get_piqa(nsamples//4, seqlen, tokenizer, seed=seed)
        winogrande_train=get_winogrande(nsamples//4, seqlen, tokenizer, seed=seed)
        train=wiki_train+boolq_train+piqa_train+winogrande_train
        val=None
        return train,val
    elif name.lower() == "mix128":
        wiki_train,wiki_val=get_wikitext2(nsamples, seed, seqlen, model, tokenizer)
        boolq_train=get_boolq(nsamples, seqlen, tokenizer, seed=seed)
        piqa_train=get_piqa(nsamples, seqlen, tokenizer, seed=seed)
        winogrande_train=get_winogrande(nsamples, seqlen, tokenizer, seed=seed)
        train=wiki_train+boolq_train+piqa_train+winogrande_train
        val=None
        return train,val